{"meta":{"title":"Fantasy的个人博客","subtitle":null,"description":null,"author":"Fantasy","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"LaLa心理学音乐会","slug":"LaLa心理学音乐会","date":"2018-03-18T15:05:06.000Z","updated":"2018-03-18T15:41:01.000Z","comments":true,"path":"2018/03/18/LaLa心理学音乐会/","link":"","permalink":"http://yoursite.com/2018/03/18/LaLa心理学音乐会/","excerpt":"上周无聊打开了大麦网，发现有徐佳莹的音乐会耶，票不贵，赶紧入手了一张。 记得最早听她是那张《寻人启事》，一个几乎没名气的新人，直接拿到个6个金曲奖的提名。金曲奖虽然有很多诟病的地方，但一直还是很喜欢它的品味的，每年都能发掘出一批不出名，但很优秀的作品。","text":"上周无聊打开了大麦网，发现有徐佳莹的音乐会耶，票不贵，赶紧入手了一张。 记得最早听她是那张《寻人启事》，一个几乎没名气的新人，直接拿到个6个金曲奖的提名。金曲奖虽然有很多诟病的地方，但一直还是很喜欢它的品味的，每年都能发掘出一批不出名，但很优秀的作品。Lala的歌走的是走心的路线，适合一个人安静时慢慢的听，比如失落沙洲和寻人启事，当时感觉发现了个宝，很长一段时间都在循环着早期的四张专辑。 后来上了我是歌手，慢慢的也被人熟知，记得首场还拿了第一，那时每期都看。作为原创歌手，这点和杰伦有点像，演绎的是别人的歌曲，但最后演绎的都成自己的啦。对于这类歌手，估计原唱内心是拒绝的哈😆。歌手上记忆很深的有《浪费》《我好想你》 《修炼爱情》 《莉莉安》 《相爱后动物感伤》。 后来听的倒越来越少啦，但关注了网易云音乐，每次出新歌新MV都会有推送，每首都会听的，出名后的LaLa讲真还是和原先差不多，这可能也注定了她不会太红。但这类歌手，喜欢她的会一直喜欢着。 音乐会果然和演唱会不同，只一个小时多一点，完全没听够，希望下次能够听场她的演唱会。","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"tornado源码学习笔记","slug":"tornado源码学习笔记","date":"2017-03-16T13:31:14.000Z","updated":"2018-03-16T14:08:00.000Z","comments":true,"path":"2017/03/16/tornado源码学习笔记/","link":"","permalink":"http://yoursite.com/2017/03/16/tornado源码学习笔记/","excerpt":"yield用法在python中，如果一个函数里面含有yield语句，那么调用函数，返回的不是函数的return结果，而是一个生成器。之后进行操作能将这个函数返回中间结果给调用者，然后维护函数的局部状态，所以当函数离开时，也能恢复执行。来看一个例子：","text":"yield用法在python中，如果一个函数里面含有yield语句，那么调用函数，返回的不是函数的return结果，而是一个生成器。之后进行操作能将这个函数返回中间结果给调用者，然后维护函数的局部状态，所以当函数离开时，也能恢复执行。来看一个例子： 123456def fib(n_max): n,a,b = 0,0,1 while n&lt;n_max: yield b a,b = b,a+b n += 1 调用函数返回的是一个生成器 12In [53]: fib(6)Out[53]: &lt;generator object fib at 0x103eb5640&gt; 想要得到结果，需要再次将其当做可迭代对象处理，例如使用for或多次使用next方法。 next和send方法含yield语句的函数返回的是一个生成器类，这个生成器自带一个next方法，每次调用x.next()时，会执行函数类的代码，直到出现了yield，程序返回yield后面的结果。程序并挂起，直到下次调用next()，程序继续执行。直到出现yield程序再次挂起。来看个例子吧。 1234567891011121314151617181920212223242526In [55]: def func(): ....: print \"aaaa\" ....: yield 1 ....: print \"bbbb\" ....: yield 2 ....: print \"cccc\" ....: yield 3 ....:In [56]: c = func()In [57]: cOut[57]: &lt;generator object func at 0x103eb5690&gt;In [58]: c.next()aaaaOut[58]: 1In [59]: c.next()bbbbOut[59]: 2In [60]: c.next()ccccOut[60]: 3In [61]: c.next()---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-61-50b4dad19337&gt; in &lt;module&gt;()----&gt; 1 c.next()StopIteration: 如果我们将yield看做表达式，并赋值给一个变量，那这个变量的得到的是什么呢。就像这样： 123456def func(): print \"start\" a = yield 1 print \"a:\", a b = yield 2 print \"b:\", b 这里a得到的并不是1，b并不是2. 12345678In [69]: c.next()startOut[69]: 1In [70]: c.next()a: NoneOut[70]: 2In [71]: c.next()b: None next方法返回的才是yield后面的内容，a,b得到的是None。生成器里面还有一个函数是send函数，调用send函数也有next函数的效果，或者其实说c.next()相当于调用c.send(None)。a,b得到的None就来自send的值。再看： 1234567891011In [79]: c = func()In [80]: r = c.next()startIn [81]: rOut[81]: 1In [82]: r2 = c.send(\"lalala\")a: lalalaIn [83]: r2Out[83]: 2In [84]: r3 = c.send(\"hahaha\")b: hahaha 注意第一次调用调用的是send(None)或next().可以看出send方法返回的是yield后面的东西，而yield赋值给的变量a,b得到的是send函数里面的内容。正式yield的这种特性可以用来实现协程，程序在某个时刻通过yield挂起当前函数，切换到其他地方运行代码，再在某个时刻调用send()。程序继续执行。例如我们有一个比较耗时的io操作,可以通过yield切换到其他程序继续执行，当io操作好了的时候，通过send通知函数继续执行，程序不会阻塞在io操作上。tornado也是借助这种方式来实现协程进行异步操作。 Future类分析Future类是tornado一个特别重要的类，tornado借助Future类保存程序异步的结果。这个结果包括返回值，异常信息，回调函数。例如刚才遇到耗时io操作时，将耗时操作包括回调等封装Future中，然后通过yield返回。通过ioloop来异步处理，最后调用Future的set_result将io操作结果放入Future中。来看看主要源码及它拥有的属性和方法吧 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Future(object): def __init__(self): self._done = False self._result = None self._exc_info = None self._callbacks = [] def running(self): \"\"\"Returns True if this operation is currently running.\"\"\" return not self._done def done(self): \"\"\"Returns True if the future has finished running.\"\"\" return self._done def result(self, timeout=None): self._clear_tb_log() if self._result is not None: return self._result if self._exc_info is not None: raise_exc_info(self._exc_info) self._check_done() return self._result def exception(self, timeout=None): \"\"\" 返回异常信息 \"\"\" self._clear_tb_log() if self._exc_info is not None: return self._exc_info[1] else: self._check_done() return None def add_done_callback(self, fn): if self._done: fn(self) else: self._callbacks.append(fn) def set_result(self, result): \"\"\"Sets the result of a ``Future``. It is undefined to call any of the ``set`` methods more than once on the same object. \"\"\" self._result = result self._set_done() def set_exception(self, exception): \"\"\"Sets the exception of a ``Future.``\"\"\" self.set_exc_info( (exception.__class__, exception, getattr(exception, '__traceback__', None))) def exc_info(self): \"\"\"省略\"\"\" def set_exc_info(self, exc_info): \"\"\"省略\"\"\" def _check_done(self): if not self._done: raise Exception(\"DummyFuture does not support blocking for results\") def _set_done(self): self._done = True for cb in self._callbacks: try: cb(self) except Exception: app_log.exception('Exception in callback %r for %r', cb, self) self._callbacks = None running()方法获取Future是否处理完得到result。如果得到返回False，没有表示还在运行，返回True. done()正好和running方法相反 result()获取Future处理完的结果，如果没有处理完，则会跑出一个DummyFuture does not support blocking for results的异常 add_done_callback(fn) 将回调函数添加到Future对象中,如果Future已经处理完，则直接执行fn，否则将fn添加到回调函数列表中，之后在执行 set_result将结果赋值给Future对象，并调用_set_done()执行回调函数 _check_done检查Future是否执行完得到结果，没有则跑出异常 _set_done设置Future为处理完，并执行回调函数列表中的函数。 Future是沟通gen模块和ioloop模块的一座桥梁。将yield语句返回的对象封装后交给ioloop去调度处理。 epoll理解epoll是对select和poll的改进版本。相对于多进程和多线程技术来说，这种I/O多路复用技术最大的优势就是减少了系统开销，通过监听等待文件操作符是否就绪，如果就绪，进程将所读数据复制到应用进程缓存区进行处理，不需要创建进程和线程，也不必维护这些进程线程。等待就绪，初步看有些像多线程中的阻塞io,但select等函数可以同时监听等待多个文件描述符，性能就大大提升啦。相对于select和poll。epoll的优点是没有监听的文件描述符的限制。且select和poll需要不断轮询所有fd(文件描述符）集合，直到设备就绪。epoll则是设备就绪时，调用回调函数，将就绪fd放入就绪表中，之后只要判断就绪表是不是为空就行啦。在Linux下系统实现了epoll模型，其他平台类似的是kqueue。 Python中epoll封装在select模块中。来看个简单的使用例子， 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import socketimport selectclass EpollServer(object): def __init__(self, host='localhost', port=8888): self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) #设置地址可重用 self.sock.bind((host, port)) self.sock.listen(1) self.sock.setblocking(0) #设置非阻塞 self.epoll = select.epoll() self.epoll.register(self.sock.fileno(), select.EPOLLIN) def run(self): try: conns = &#123;&#125; reqs = &#123;&#125; resps = &#123;&#125; while True: events = self.epoll.poll(1) for fileno, event in events: if fileno == self.sock.fileno(): conn,addr = self.sock.accept() conn_fileno = conn.fileno() conn.setblocking(0) self.epoll.register(conn.fileno, select.EPOLLIN) conns[conn_fileno] = conn reqs[conn_fileno] = b'' resps[conn_fileno] = b\"\"\"HTTP/1.0 200 OK\\r\\nDate: Mon, 1 Jan 1996 01:01:01 GMT\\r\\n Content-Type: text/plain\\r\\nContent-Length: 13\\r\\n\\r\\nHello, world!\"\"\" elif event &amp; select.EPOLLIN: reqs[fileno]+=conns[fileno].recv(1024) if b'\\n\\n' in reqs[fileno] or b'\\n\\r\\n' in reqs[fileno]: self.epoll.modify(fileno, select.EPOLLOUT) print(reqs[fileno].decode()) elif event &amp; select.EPOLLOUT: len_write = conns[fileno].send(resps[fileno]) resps[fileno] = resps[fileno][len_write:] if len(resps[fileno]) == 0: self.epoll.modify(fileno, 0) conns[fileno].shutdown(socket.SHUT_RDWR) elif event &amp; select.EPOLLHUB: self.epoll.unregister(fileno) conns[fileno].close() del conns[fileno] finally: self.epoll.unregister(self.sock.fileno()) self.epoll.close() self.sock.close()if __name__ == '__main__': epoll_server = EpollServer() epoll_server.run() 程序摘自《Python网络编程攻略》。调用select.epoll()实例化后，通过register函数来登记一个监听的文件描述符和事件。进入while循环，poll()方法返回监听到的事件event和文件描述符列表。 如果是服务器socket事件，建立连接，设置非阻塞，添加监听读事件。 如果事件event是可读，则读取数据，某个请求读完后，更新(调用modify)监听事件为监听是否可写。 如果事件event是可写，发送需要响应回的数据，发送完后,更改监听事件， 如果文件描述符被挂断， 调用unregister不再监听此文件描述符，关闭socket连接，并删除。 主要的几个方法就是poll,register,modify,unregister,close。 tornado在linux下是通过epoll来实现I/O多路复用。对io事件进行监听，ioloop ioloop模块分析ioloop是tornado底层的核心，主要调度处理io读写错误事件以及回调和超时等任务。当程序碰到yield切换时，便将yield后面的Future交给iooop调度处理，程序切换执行其他代码。 Configurable类 ioloop模块中，重要的是IOLoop类，它继承自Configurable类，Configurable是个抽象类，它负责让IOLoop实例化对象时，会根据不同的平台，实例成不同的对象，例如Linux实例化之后就可以调用EPollIOLoop类的所有方法。正如我们一般通过tornado.ioloop.IOLoop.instance().start()来启动IOLoop,但其实在Linux下，启动的是EpollIOLoop实例，执行的也是里面的方法。看看源码可能清晰许多 12345678910111213141516def __new__(cls, *args, **kwargs): base = cls.configurable_base() init_kwargs = &#123;&#125; if cls is base: impl = cls.configured_class() if base.__impl_kwargs: init_kwargs.update(base.__impl_kwargs) else: impl = cls init_kwargs.update(kwargs) instance = super(Configurable, cls).__new__(impl) # initialize vs __init__ chosen for compatibility with AsyncHTTPClient # singleton magic. If we get rid of that we can switch to __init__ # here too. instance.initialize(*args, **init_kwargs) return instance 在Linux平台下，configurable_base()返回一个IOLoop类,configured_class()返回的是EPollIOLoop类。继承了Configurable类的类实例化时，返回的实例在Linux下其实是一个EPollIOLoop实例，同理，如果系统使用的是kqueue，实例则是一个KQueueIOLoop实例 IOLoop类 IOloop比较复杂，主要负责调度。通过源码来看看一些常见的方法。我们通常调用tornado.ioloop.IOLoop.current().start()来启动实例，current()会判断当前有没有实例instance，如果没有，则调用instance()方法创建一个实例。instance()采用了单例模式，即使多次调用，创建的也是同一个全局IOLoop实例。 12345678910@staticmethoddef instance(): \"\"\"Returns a global `IOLoop` instance. \"\"\" if not hasattr(IOLoop, \"_instance\"): with IOLoop._instance_lock: if not hasattr(IOLoop, \"_instance\"): # New instance after double check IOLoop._instance = IOLoop() return IOLoop._instance 实例创建后，需要通过start()方法来启动。这个方法实现在子类PollIOLoop类中。也是这个模块中最复杂的一个函数。来看主要代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182while True: #获取锁，然后将回调函数列表清空 with self._callback_lock: callbacks = self._callbacks self._callbacks = [] due_timeouts = [] #如果有超时任务,且已经超时，处理超时任务。还未超时，添加到due_timeouts列表中. if self._timeouts: now = self.time() while self._timeouts: if self._timeouts[0].callback is None: # The timeout was cancelled. Note that the # cancellation check is repeated below for timeouts # that are cancelled by another timeout or callback. heapq.heappop(self._timeouts) self._cancellations -= 1 elif self._timeouts[0].deadline &lt;= now: due_timeouts.append(heapq.heappop(self._timeouts)) else: break if (self._cancellations &gt; 512 and self._cancellations &gt; (len(self._timeouts) &gt;&gt; 1)): # Clean up the timeout queue when it gets large and it's # more than half cancellations. self._cancellations = 0 self._timeouts = [x for x in self._timeouts if x.callback is not None] heapq.heapify(self._timeouts) #处理回调函数 for callback in callbacks: self._run_callback(callback) #处理快要超时的超时任务 for timeout in due_timeouts: if timeout.callback is not None: self._run_callback(timeout.callback) # Closures may be holding on to a lot of memory, so allow # them to be freed before we go into our poll wait. callbacks = callback = due_timeouts = timeout = None #如果刚清空的_callbacks又有回调任务或超时回调任务添加进来，调用他们之前不在poll(poll_timeout)中等待下一次轮询 if self._callbacks: poll_timeout = 0.0 elif self._timeouts: # If there are any timeouts, schedule the first one. # Use self.time() instead of 'now' to account for time # spent running callbacks. poll_timeout = self._timeouts[0].deadline - self.time() poll_timeout = max(0, min(poll_timeout, _POLL_TIMEOUT)) else: # No timeouts and no callbacks, so use the default. poll_timeout = _POLL_TIMEOUT if not self._running: break if self._blocking_signal_threshold is not None: # clear alarm so it doesn't fire while poll is waiting for # events. signal.setitimer(signal.ITIMER_REAL, 0, 0) try: #开始监听事件 event_pairs = self._impl.poll(poll_timeout) except Exception as e: if errno_from_exception(e) == errno.EINTR: continue else: raise self._events.update(event_pairs) 如果监听到事件，处理事件 while self._events: fd, events = self._events.popitem() try: fd_obj, handler_func = self._handlers[fd] handler_func(fd_obj, events) except (OSError, IOError) as e: if errno_from_exception(e) == errno.EPIPE: # Happens when the client closes the connection pass else: self.handle_callback_exception(self._handlers.get(fd)) except Exception: self.handle_callback_exception(self._handlers.get(fd)) fd_obj = handler_func = None 主要做的就是监听事件，处理事件，以及处理回调函数以及超时回调函数，注意这里的超时回调函数表是一个最小堆，根据需要在某个时刻执行根据时间对成了堆，每次从堆中拿出来的第一个任务就是离现在最近的任务，从而实现延时处理的效果。 接下来来看几个重要的函数: add_handler(fd,handler,events)函数 1234def add_handler(self, fd, handler, events): fd, obj = self.split_fd(fd) self._handlers[fd] = (obj, stack_context.wrap(handler)) self._impl.register(fd, events | self.ERROR) fd为一个文件操作符，handler为一个处理函数，events为io事件，这个函数使用epoll登记监听fd上的events，当事件发生时，调用handler(fd,events处理事件) add_timeout(deadline,callback,args,*kwargs)函数。在deadline的时候调用callback函数。这个函数调用的是实现在PollIOLoop类中的call_at函数，call_at函数会将任务封装后添加到start()中看到过的self._timeouts堆中。 add_callback(callback,args,*kwargs)函数,也是在PollIOLoop中实现 123456789101112131415def add_callback(self, callback, *args, **kwargs): if thread.get_ident() != self._thread_ident: with self._callback_lock: if self._closing: return list_empty = not self._callbacks self._callbacks.append(functools.partial( stack_context.wrap(callback), *args, **kwargs)) if list_empty: self._waker.wake() else: if self._closing: return self._callbacks.append(functools.partial( stack_context.wrap(callback), *args, **kwargs) 也是在PollIOLoop中实现，将任务通过偏函数包装后，添加到回调函数列表中。在之后的循环中处理。 add_future(future,callback)函数 12345678910def add_future(self, future, callback): \"\"\"Schedules a callback on the ``IOLoop`` when the given `.Future` is finished. The callback is invoked with one argument, the `.Future`. \"\"\" assert is_future(future) callback = stack_context.wrap(callback) future.add_done_callback( lambda future: self.add_callback(callback, future)) 添加一个callback函数到Future对象中，当Future对象被set_result,执行一个回调函数，即这个lambda函数，在lambda函数中调用IOLoop的add_callback函数。将add_future的参数callback加入到IOLoop的统一调度中，让callback在IOLoop下一次迭代中执行。 run_sync(func,timeout=None)函数 iostream模块分析iostream模块中IOStream主要是对socket读写进行封装。类初始化时，会绑定一个socket，绑定一个ioloop。然后分别创建一个读缓冲区_read_buffer和一个写缓冲区_write_buffer。这两个缓存区都是collections.deque()双端队列类型。 读写接口 read_until(delimiter,callback=None,max_bytes=None)异步从缓冲区读取数据直到读取到了分隔符delimiter。如果有callback函数，读取到数据后执行回调函数，如果缓冲区没有数据，则需要先调用_read_to_buffer()从socket读取数据，然后将数据写入buffer中。如果max_bytes给了，如果没有碰到delimiter，则会一直读取数据，超过max_bytes后会关闭连接。 read_until_regex(regex,callback=None,max_bytes=None)和read_until类似，不同的是将delimiter换成了一个正则表达式，从缓冲区读取数据直到读取到的数据满足正则表达式 read_bytes(num_bytes, callback=None, streaming_callback=None,partial=False)异步读取长度为num_bytes的数据。 read_until_close(callback=None, streaming_callback=None) 异步读取数据直到socket关闭 write(data,callback=None) 将数据写入缓存区中。 几个主要的内部函数： write_to_fd(data) 将数据写入socket中，发送出去 read_from_fd 从socket读取数据 _read_to_buffer 读取socket的数据并添加到缓冲区buffer中。 _read_from_buffer从buffer中读取数据 _try_inline_read()试着从buffer读取数据，如果数据没有或不够，则开始监听socket，读取新的数据 _add_io_state(state) 在ioloop中注册事件状态，调用的是ioloop的add_handler函数，监听socket，当满足state状态时，调用self._handle_events处理事件。 还有一个比较有意思额的函数是_consume(loc)，他能将缓冲区的前loc个数据合并，例如[‘abc’,’de’,’fgh’,’i’]合并前6个数据后变成了[‘abcdef’,’gh’,’i’]然后调用popleft()返回这loc字节数据 gen.coroutine装饰器分析coroutine装饰器主要作用是将，本来需要异步回调写的代码，看起来像是在写同步函数。例如抓取一个网页，回调写法是: 12345678910111213from tornado.httpclient import AsyncHTTPClientimport tornado.ioloopdef callback(response): do_something(response)def asychronous_fetch(url, callback): httpclient = AsyncHTTPClient() def handle_response(response): callback(response.body) httpclient.fetch(url,callback=handle_response)if __name__ == '__main__': url = 'http://www.baidu.com' asychronous_fetch(url, callback) tornado.ioloop.IOLoop.instance().start() 通过coroutine装饰器，代码写起来是这样的 123456789101112from tornado import genfrom tornado.httpclient import AsyncHTTPClientimport tornadourl = 'http://www.baidu.com'@gen.coroutinedef fetch_cor(url): http_client = AsyncHTTPClient() response = yield http_client.fetch(url) do_something(response.body)if __name__ == '__main__': fetch_cor(url) tornado.ioloop.IOLoop.instance().start() 直接调用函数返回的是一个生成器，装饰器调用next()，遇到yield挂起，装饰器将yield后面的对象进行异步处理，处理完之后将结果通过send发送回来，也就被response接收。看起来像是同步的代码进行的是异步处理。 装饰器主要源码: 123456789101112131415161718192021222324252627282930313233343536373839def _make_coroutine_wrapper(func, replace_callback): if hasattr(types, 'coroutine'): func = types.coroutine(func) @functools.wraps(func) def wrapper(*args, **kwargs): future = TracebackFuture() if replace_callback and 'callback' in kwargs: callback = kwargs.pop('callback') IOLoop.current().add_future( future, lambda future: callback(future.result())) try: result = func(*args, **kwargs) except: '''省略''' else: if isinstance(result, GeneratorType): try: orig_stack_contexts = stack_context._state.contexts #初次调用next,获取yield返回的对象 yielded = next(result) if stack_context._state.contexts is not orig_stack_contexts: yielded = TracebackFuture() yielded.set_exception( stack_context.StackContextInconsistentError( 'stack_context inconsistency (probably caused ' 'by yield within a \"with StackContext\" block)')) except (StopIteration, Return) as e: future.set_result(_value_from_stopiteration(e)) except Exception: future.set_exc_info(sys.exc_info()) else: Runner(result, future, yielded) try: return future finally: future = None future.set_result(result) return future return wrapper result为直接调用函数返回的装饰器，之后调用next()得到的yielded为yield语句返回的对象。之后放入Runner类初始化过程中统一处理，处理完后的send()也在Runner类中执行。 Runner类Runner实例化时，会调用两个重要的函数。handle_yield(yielded)和run()函数。handle_yield函数将第一次调用next()返回的yielded进行处理。handle_yield的部分代码 1234567891011try: #转化为Future self.future = convert_yielded(yielded)except BadYieldError: self.future = TracebackFuture() self.future.set_exc_info(sys.exc_info())if not self.future.done() or self.future is moment: self.io_loop.add_future( self.future, lambda f: self.run()) return Falsereturn True 将yielded对象转化为Future对象，然后调用add_furure()将回调函数(这里为lambda)和future对象加入ioloop调度，如果future还没执行完得到结果，则需要等set_result()设置result后，才会执行这个回调函数(run函数)。如果执行完得到结果handle_yield()返回True,否则返回False。如果返回True，则调用run()函数。来看看run函数 1234567891011121314151617181920212223242526272829303132333435363738def run(self): \"\"\"Starts or resumes the generator, running until it reaches a yield point that is not ready. \"\"\" if self.running or self.finished: return try: self.running = True while True: future = self.future if not future.done(): return self.future = None try: orig_stack_contexts = stack_context._state.contexts exc_info = None try: value = future.result() except Exception: self.had_exception = True exc_info = sys.exc_info() if exc_info is not None: yielded = self.gen.throw(*exc_info) exc_info = None else: #将结果发送回 yielded = self.gen.send(value) if stack_context._state.contexts is not orig_stack_contexts: self.gen.throw( stack_context.StackContextInconsistentError( 'stack_context inconsistency (probably caused ' 'by yield within a \"with StackContext\" block)')) except: \"\"\"省略\"\"\" if not self.handle_yield(yielded): return finally: self.running = False 刚才说只有Future对象处理完，拿到result,才调用run().run()中将result发送回去，比如上面例子中的response变量。 httpserver模块分析tornado多进程","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"图的深度优先和广度优先遍历-Python实现","slug":"图的深度优先和广度优先遍历-Python实现","date":"2017-01-16T15:04:55.000Z","updated":"2018-03-16T15:08:03.000Z","comments":true,"path":"2017/01/16/图的深度优先和广度优先遍历-Python实现/","link":"","permalink":"http://yoursite.com/2017/01/16/图的深度优先和广度优先遍历-Python实现/","excerpt":"图在Python中可以表示为邻接矩阵,也可移用字典的形式来表示,就像这样: 1234567graph = &#123;'A': ['B','C'], 'B': ['D','E'], 'C': ['F','G'], 'D': [], 'E': [], 'F':[], 'G':[]&#125; 表示的图为&#39;A&#39;-&gt;&#39;B&#39;,&#39;A&#39;-&gt;&#39;C&#39;,&#39;B&#39;-&gt;&#39;D&#39;,&#39;B&#39;-&gt;&#39;E&#39;,&#39;C&#39;-&gt;&#39;F&#39;,&#39;C&#39;-&gt;&#39;G&#39;","text":"图在Python中可以表示为邻接矩阵,也可移用字典的形式来表示,就像这样: 1234567graph = &#123;'A': ['B','C'], 'B': ['D','E'], 'C': ['F','G'], 'D': [], 'E': [], 'F':[], 'G':[]&#125; 表示的图为&#39;A&#39;-&gt;&#39;B&#39;,&#39;A&#39;-&gt;&#39;C&#39;,&#39;B&#39;-&gt;&#39;D&#39;,&#39;B&#39;-&gt;&#39;E&#39;,&#39;C&#39;-&gt;&#39;F&#39;,&#39;C&#39;-&gt;&#39;G&#39; 接下来来看看图的深度优先和广度优先遍历 深度优先过程: 1 访问初始结点v，并标记结点v为已访问。2 查找结点v的第一个邻接结点w。3 若w存在，则继续执行4，否则算法结束。4 若w未被访问，对w进行深度优先遍历递归（即把w当做另一个v，然后进行步骤123）。5 查找结点v的w邻接结点的下一个邻接结点，转到步骤3。 广度优先过程: 1 访问初始结点v并标记结点v为已访问。2 结点v入队列3 当队列非空时，继续执行，否则算法结束。4 出队列，取得队头结点u。5 查找结点u的第一个邻接结点w。6 若结点u的邻接结点w不存在，则转到步骤3；否则循环执行以下三个步骤： 1). 若结点w尚未被访问，则访问结点w并标记为已访问。 2). 结点w入队列 3). 查找结点u的继w邻接结点后的下一个邻接结点w，转到步骤6。 代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243class Grapy(object): \"\"\"docstring for Grapy\"\"\" def __init__(self, *arg,graph=&#123;&#125;): self.graph = graph self.visited = &#123;&#125; def depth_first_traverse(self,root=None): result=[] def dft(node): self.visited[node]=True result.append(node) for n in self.graph[node]: if not n in self.visited: dft(n) if root: dft(root) return result def breadth_first_traverse(self,root=None): queue = [root] result = [root] def bft(): while queue: node = queue.pop(0) self.visited[node] = True for n in self.graph[node]: if (not n in self.visited) and (not n in queue): queue.append(n) result.append(n) if root: bft() return resultgraph = &#123;'A': ['B','C'], 'B': ['D','E'], 'C': ['F','G'], 'D': [], 'E': [], 'F':[], 'G':[]&#125;gd = Grapy(graph=graph)red = gd.depth_first_traverse('A')gb = Grapy(graph=graph)reb = gb.breadth_first_traverse('A')print('depth first traverse:',red)print('breadth first traverse:',reb) 结果:123depth first traverse: ['A', 'B', 'D', 'E', 'C', 'F', 'G']breadth first traverse: ['A', 'B', 'C', 'D', 'E', 'F', 'G'][Finished in 0.0s]","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"快速排序-Python实现","slug":"快速排序-Python实现","date":"2016-12-16T14:52:32.000Z","updated":"2018-03-16T14:53:54.000Z","comments":true,"path":"2016/12/16/快速排序-Python实现/","link":"","permalink":"http://yoursite.com/2016/12/16/快速排序-Python实现/","excerpt":"快速排序是一种高效的排序算法,是一种对冒泡排序算法的改进,尽管最坏情况下时间复杂度也是O(n^2),但从平均效率来看,时间复杂为O(nlgn).所以如名字一样,速度很快.","text":"快速排序是一种高效的排序算法,是一种对冒泡排序算法的改进,尽管最坏情况下时间复杂度也是O(n^2),但从平均效率来看,时间复杂为O(nlgn).所以如名字一样,速度很快. 快速排序的主要思想是分治思想.一半的步骤是: 从列表中任意选取一个数为基准数,栗子中选取列表的第一个数为基准数将列表右边开始遍历,遇到比基准数小的数停一下,在从左边开始遍历,遇到比基准数小的数停一下,将这两个数对换一下,如6 2 7 9 3 4 5 10 8,基准数为6,第一次对换将5和7对换,就变成了6 1 2 5 9 3 4 7 10 8如次重复,将比6大的放左边,比6小的放左边,6大的放右边,类似于这样:3 1 2 5 4 6 9 7 10 8运用递归,将6的左边重复第二步,6的右边重复第二步.知道各区间只有一个数,排序也就完成.代码: 1234567891011121314151617181920212223242526import randomdef qsort(arr, start, end): base = arr[start] i = start j = end while i &lt; j: while i &lt; j and arr[j] &gt;= base: j -= 1 while i &lt; j and arr[i] &lt;= base: i += 1 if(i&lt;j): arr[i], arr[j] = arr[j], arr[i] else: break arr[start] = arr[i] arr[i]=base if i - 1 &gt; start: qsort(arr, start, i - 1) if j + 1 &lt; end: qsort(arr, j + 1, end) a = [] for i in range(12): a.append(random.randint(0, 100)) print('Before quick sorted ',a) qsort(a,0,len(a)-1) print('After quick sorted: ',a) 结果: 12Before quick sorted [70, 83, 89, 72, 38, 92, 55, 64, 43, 57, 86, 73]After quick sorted: [38, 43, 55, 57, 64, 70, 72, 73, 83, 86, 89, 92]","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"二叉树遍历及计算深度-Python实现","slug":"二叉树遍历及计算深度-Python实现","date":"2016-12-11T15:00:56.000Z","updated":"2018-03-16T15:07:29.000Z","comments":true,"path":"2016/12/11/二叉树遍历及计算深度-Python实现/","link":"","permalink":"http://yoursite.com/2016/12/11/二叉树遍历及计算深度-Python实现/","excerpt":"递归实现:","text":"递归实现: 代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# -*- coding:utf-8 -*-class Node(object): def __init__(self, data=None, left=None, right=None): self.data = data self.left = left self.right = rightclass BinTree(object): \"\"\"docstring for BinTree\"\"\" def __init__(self, root): self.root = root # 前序遍历 def pretraver(self, treenode): if not treenode: return print(treenode.data) self.pretraver(treenode.left) self.pretraver(treenode.right) # 中序遍历 def midtraver(self, treenode): if not treenode: return self.midtraver(treenode.left) print(treenode.data) self.midtraver(treenode.right) # 后序遍历 def aftertraver(self, treenode): if not treenode: return self.aftertraver(treenode.left) self.aftertraver(treenode.right) print(treenode.data) # 计算深度 def getdepth(self, treenode): if not treenode: return 0 left = 1 right = 1 left += self.getdepth(treenode.left) right += self.getdepth(treenode.right) if left &gt;= right: return left else: return right # 统计节点数: def getsize(self, treenode): if not treenode: return 0 result = self.getsize(treenode.left) + self.getsize(treenode.right) + 1 return resultif __name__ == '__main__': root = Node('D', Node('B', Node('A'), Node('C')), Node('E', right=Node('G', Node('F', Node('H'))))) bt = BinTree(root) print('prev:') bt.pretraver(bt.root) print('mid:') bt.midtraver(bt.root) print('after:') bt.aftertraver(bt.root) print('Depth of the tree') depth = bt.getdepth(bt.root) print(depth) print('summer of nodes:') number = bt.getsize(bt.root) print(number)``` ##### 非递归实现:###### 前序算法步骤:1 访问结点p,并将p入栈2 判断p的左结点是否为空,若为空,则取栈顶结点,并将其出栈,然后将栈顶结点的右结点为p,并循环至1.若不为空,则将p的左结点置为当前结点3 直到p为None 且栈为空,则遍历结束代码:```pydef pretraver(root): p = root s = [] while p or s: while p: print(p.data) s.append(p) p = p.left if s: p = s.pop() p = p.right 中序根据中序遍历的顺序，对于任一结点，优先访问其左孩子，而左孩子结点又可以看做一根结点，然后继续访问其左孩子结点，直到遇到左孩子结点为空的结点才进行访问，然后按相同的规则访问其右子树。因此其处理过程如下：对于任一结点P，1)若其左孩子不为空，则将P入栈并将P的左孩子置为当前的P，然后对当前结点P再进行相同的处理；2)若其左孩子为空，则取栈顶元素并进行出栈操作，访问该栈顶结点，然后将当前的P置为栈顶结点的右孩子；3)直到P为NULL并且栈为空则遍历结束中序 代码: 1234567891011def midtraver(root): p = root s = [] while p or s: while p: s.append(p) p=p.left if s: p = s.pop() print(p.data) p = p.right 后序:要保证根结点在左孩子和右孩子访问之后才能访问，因此对于任一结点P，先将其入栈。如果P不存在左孩子和右孩子，则可以直接访问它；或者P存在左孩子或者右孩子，但是其左孩子和右孩子都已被访问过了，则同样可以直接访问该结点。若非上述两种情况，则将P的右孩子和左孩子依次入栈，这样就保证了每次取栈顶元素的时候，左孩子在右孩子前面被访问，左孩子和右孩子都在根结点前面被访问。 代码: 1234567891011121314151617def aftertraver(root): s = [] cur = None #当前结点 pre = None #前一次访问的结点 s.append(root) while s: cur = s[-1] if ((not cur.left) and (not cur.right)) or (pre and (pre==cur.left or pre==cur.right)): print(cur.data) s.pop() pre = cur else: if cur.right: s.append(cur.right) if cur.left: s.append(cur.left)","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"归并排序-Python实现","slug":"归并排序-Python实现","date":"2016-11-16T14:55:04.000Z","updated":"2018-03-16T14:56:27.000Z","comments":true,"path":"2016/11/16/归并排序-Python实现/","link":"","permalink":"http://yoursite.com/2016/11/16/归并排序-Python实现/","excerpt":"归并排序也是分治思想的一个典型应用,思路是先将数列递归分解,如将数列分解成A,B.再将A,B各自分解成两个数列,依次类推,当分出来的小组只有一个数时，可以认为小组组内已经有序啦．然后就是考虑怎么将两个有序数组合并．我们定义一个第三方数组Ｃ,","text":"归并排序也是分治思想的一个典型应用,思路是先将数列递归分解,如将数列分解成A,B.再将A,B各自分解成两个数列,依次类推,当分出来的小组只有一个数时，可以认为小组组内已经有序啦．然后就是考虑怎么将两个有序数组合并．我们定义一个第三方数组Ｃ, 遍历比较两数组，谁小就取谁，放入Ｃ数组中，取完往后移一个再比较．如果某个数组取完啦，把另外一个数组剩下的全部放进Ｃ数组即可． 代码 12345678910111213141516171819202122232425262728293031import random#合并函数def merge(a,b): c = [] i=j=0 while i&lt;len(a) and j&lt;len(b): if a[i]&lt;b[j]: c.append(a[i]) i+=1 else: c.append(b[j]) j+=1 if i&lt;len(a): c+=a[i:] if j&lt;len(b): c+=b[j:] return cdef mergesort(arr): if len(arr)&lt;=1: return arr mid = int(len(arr)/2) a = mergesort(arr[:mid]) b = mergesort(arr[mid:]) return merge(a,b)if __name__ == '__main__': arr = [] for i in range(15): arr.append(random.randint(0, 100)) print('Before merge sorted ',arr) result = mergesort(arr) print('After merge sorted: ',result) 结果： 12Before merge sorted [53, 100, 26, 47, 3, 31, 40, 34, 75, 67, 66, 10, 29, 70, 73]After merge sorted: [3, 10, 26, 29, 31, 34, 40, 47, 53, 66, 67, 70, 73, 75, 100]","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"Python并发编程之线程总结","slug":"Python并发编程之线程总结","date":"2016-11-16T14:09:38.000Z","updated":"2018-03-16T14:15:41.000Z","comments":true,"path":"2016/11/16/Python并发编程之线程总结/","link":"","permalink":"http://yoursite.com/2016/11/16/Python并发编程之线程总结/","excerpt":"一个正在运行的程序可以成为一个进程,每个进程管理者他自己的系统状态.比如打开一个播放器来听听歌,播放器这个运行的程序就是一个进程.","text":"一个正在运行的程序可以成为一个进程,每个进程管理者他自己的系统状态.比如打开一个播放器来听听歌,播放器这个运行的程序就是一个进程.进程之间是相互独立的,但感觉一个听歌时,我们一边听着歌声,一边看看歌词,一边还刷刷评论,仿佛有几个任务在同时运行着,这一个个的子任务,就是一个个进程创建出来的线程,当然,程序要运行,至少要有一个线程.进程和线程在操作系统的调度下,不断的切换,由于时间极短,感觉是在同时运行的.多个程序同时运行,也叫并行,真正的并行需要运行在多核CPU上,正因为进程和线程的不断切换,操作系统也会自动把很多任务轮流调度到每个核心上执行,在早期的单核CPU电脑时,我们也可以同时运行多个程序或任务.在多线程编程中,多个线程同时更新同一个数据结构时容易造成资源竞争,这时需要引入互斥锁等机制来保持数据的同步.比如多个线程往一个文件里写东西,多个线程同时写,那不乱套了吗,需要锁来保持一次只有一个线程在写入,多个线程依次执行. 这里再说一下并行和并发的区别,并行是同时运行,只有在多核cpu上才能真正实现,而并发是指在一段时间间隔内,多个事件发生,就好比做好吃的时,我可以先炒炒菜,然后洗去那个调味品,回来继续炒,然后又去洗洗下一个菜的材料,然后又翻两下锅里.一个时间间隔内,切换着做了多件事,其实同一个时间点,只做一件事.在网络服务器上,并发也指能同时处理的连接数,比如一个服务器能同时处理1000个TCP链接,这个服务器的并发量就是1000,尽管服务器可能只是双核或4核的. Python在大部分系统上都支持不同形式的并发编程.一般为三种:多进程,多线程,协程.对于多进程,还没有去好好研究,先总结一下多线程和协程吧. 多线程 大多数人对多线程比较熟悉,Java等语言都支持的很好,可以将多个线程同时运行在多个核心上,并保证线程安全.蛋疼的是Python因为历史原因,多线程机制有着限制,为了保证运行线程安全,Cpython解释器(GIL)引入了内部全局解释器锁这样一个东西,而Cpython是大部分环境下默认的Python程序执行环境,GIL同时只允许一个线程执行,哪怕程序在多核处理器的计算机上也只能在运行在单个处理器上,这也是GIL争论最多的地方,当然多线程尽管在性能上不太行,但它还是有它可以并发执行程序的优点.而且Python提供的threading等模块也使写多线程程序变得非常简单,这也就是Python的风格.来看看多线程的用法吧! 最最最简单的: 123456import threadingdef hello(): print('hello')t = threading.Thread(target=hello)t.start()t.join() 程序运行在主线程上,创建一个Thread实例也就是创建一个新线程,target参数传入需要通过新线程处理的函数名.通过新线程的start方法启动这个新线程,新线程的函数开始执行.join方法会等待调用join的线程执行完后,再回到主线程继续执行.当然join还可以传入一个Timeout,像这样t.join(0.5),启动线程后,在0.5s的时间内,如果线程未执行完,也会终止这个线程,跳入下一个线程或主线程. 也可以通过重载Thread的run方法来达到同样的目的.这样 123456import threadingclass MyThread(threading.Thread): def run(self): print('我在新线程中哟!')t = MyThread()t.start() 线程在并发时运行效率可以大大的提高,看下面这个栗子:单线程: 12345678910111213import threadingimport timedef thfun(): s = 0 for i in range(30): s += i time.sleep(0.1) print(s)if __name__ == '__main__': t = time.time() thfun() thfun() print('单线程运行时间:',time.time()-t) 运行结果: 1234435435单线程运行时间: 6.008038520812988[Finished in 6.0s] 多线程: 12345678910111213141516import threadingimport timedef thfun(): s = 0 for i in range(30): s += i time.sleep(0.1) print(s)if __name__ == '__main__': ths = [threading.Thread(target=thfun) for i in range(2)] for th in ths: th.start() t = time.time() for th in ths: th.join() print('多线程运行时间:',time.time()-t) 运行结果: 1234435435多线程运行时间: 3.0041489601135254[Finished in 3.1s] 进阶守护线程:如果将线程a设置为守护线程,设置方法有a.daemon=True和a.setDaemon(True).在主线程结束时,无论a线程是否结束,都会随主线程一起退出.程序运行中，执行一个主线程，如果主线程又创建一个子线程，主线程和子线程就分兵两路，分别运行，那么当主线程完成想退出时，会检验子线程是否完成。如果子线程未完成，则主线程会等待子线程完成后再退出。但是有时候我们需要的是，只要主线程完成了，不管子线程是否完成，都要和主线程一起退出，这时就可以用setDaemon方法了。来看看栗子吧:程序: 12345678910111213141516import threading,timedef daefun(): print('daemon start...') time.sleep(2) print('daemon end.')def ndaefun(): print('ndaemon start...') time.sleep(1) print('ndaemon end.')d = threading.Thread(target=daefun)d.daemon = Truen = threading.Thread(target=ndaefun)print('main start...')d.start()n.start()print('main end.') 结果: 123456main start...daemon start...ndaemon start...main end.ndaemon end.[Finished in 1.0s 并没有看到打印’daemon end!’因为主程序执行完后,守护线程还没执行完,也跟着结束啦,有点感觉和join()方法相反,而非守护线程,在主程序结束时,依然会执行完. 线程互斥锁线程锁的作用是保证在多线程中,让同一块数据在同一时刻只能被一个线程操作,要不数据就容易乱套,所以引入锁的机制.就好比一个全封信箱,很多人想取信或放信.为了防止出错,一次只能去一个人,于是这个人蹦蹦跳跳的去取了要是,开完锁完成操作以后,将信箱锁上,再将要是放回原处,下一个人有蹦蹦跳跳的去取钥匙,打开信箱,操作.代码: 123456789101112131415161718192021import threading,timeshare = 4class MyThread(threading.Thread): def __init__(self,i): super().__init__() self.i = i def run(self): global share for d in range(3): lock.acquire() print(share) share += self.i time.sleep(0.3) print('+',self.i,'=',share) lock.release()lock = threading.Lock()if __name__ == '__main__': t = MyThread(2) tt = MyThread(6) t.start() tt.start() 运行结果: 123456789101112134+ 2 = 66+ 2 = 88+ 2 = 1010+ 6 = 1616+ 6 = 2222+ 6 = 28[Finished in 2.9s] 操作数据时,县通过acquire()方法获取锁,获取后在释放之前,因为锁只有一把,其他线程无法操作这块数据.直到通过release()方法释放锁,其他线程才能获得锁,操作数据.锁机制固然好,但要注意避免死锁.在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源，就会造成死锁 Condition对象Python提供的Condition对象提供了对复杂线程同步问题的支持。Condition被称为条件变量，除了提供与Lock类似的acquire和release方法外，还提供了wait和notify方法。 栗子:生产者通过判断是否生产,消费者通过判断是否消费. 123456789101112131415161718192021222324252627282930313233343536import threading,timeshare = 0share_cond = threading.Condition()class ProThread(threading.Thread): def __init__(self,): super().__init__() self.name = 'Produce' def run(self): global share if share_cond.acquire(): while True: if not share: share += 1 print(self.name,share) share_cond.notify() share_cond.wait() time.sleep(0.3)class CustomThread(threading.Thread): def __init__(self): super().__init__() self.name = 'Custom' def run(self): global share if share_cond.acquire(): while True: if share: share -= 1 print(self.name,share) share_cond.notify() share_cond.wait() time.sleep(0.3)if __name__ == '__main__': t = ProThread() t.start() tt = CustomThread() tt.start() 结果: 123456789101112131415Produce 1Custom 0Produce 1Custom 0Produce 1Custom 0Produce 1Custom 0Produce 1Custom 0Produce 1Custom 0Produce 1Custom 0.....","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"numpy笔记","slug":"numpy学习笔记","date":"2016-10-11T14:09:38.000Z","updated":"2018-03-16T14:18:22.000Z","comments":true,"path":"2016/10/11/numpy学习笔记/","link":"","permalink":"http://yoursite.com/2016/10/11/numpy学习笔记/","excerpt":"记录一下numpy一些函数的使用","text":"记录一下numpy一些函数的使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991:指定数据类型：dtype=''2:改变数组维度 a:如：b=np.arange(24).reshape(2,3,4) b:展平，变成一维 b.ravel() 或 b.flatten() flatten会请求分配内存来保存结果 c:转置 b.transpose() 或b.T d:b.resize((2,12))和reshape()3:数组的组合：a:水平组合：np.hstack((a,b))或np.concatenate((a,b),axis=1) b:垂直组合：np.vstack((a,b))或npconcatenate((a,b),axis=0) c:深度组合：np.dstack((a,b)4:数组分割：a:水平分割：np.hsplit(a,3) 或 np.split(a,3,axis=1) 分割成三列 b:垂直分割：np.vsplit(a,3) 或np.split(a,3,axis=0) c:深度分割：np.dsplit(c,3)5:数组的属性：.ndim属性 列出数组的纬度 .size属性 列出数组元素的总个数 .itemsize 元素在内存中所占的字节数 .nbytes 数组所占存储空间 .T 转置 .real 复数数组的实部 .imag 复数数组的虚部6：常用函数 a: np.eye(3) 单位矩阵： b: np.savetxt(\"eye.txt\",a) 数据存储 c: c,v = np.loadtxt('data.csv',delimiter=',',usecols=(6,7),unpack=True)读入CSV文件： d: np.average(c,weights=v)加权平均数 e: np.mean(c) 算数平均数 f: np.max() np.min() 最大值最小值 g: np.ptn()数组范围，即最大值最小值之差： h: np.median()中位数： i: np.msort()排序： j: np.var()方差： k: np.std()标准差： l: np.diff()数组相邻元素的差值构成的数组 m: np.log()自然对数In : n: np.where(c&gt;2)筛选: 根据指定的条件返回所有满足条件的数组元素的索引值 o: np.take(m,indexarray)获取索引值相对应的元素质： p: np.argmax 和 np.argmin 返回数组中最大和最小元素的索引值 q: np.maximum(a,b,c)返回abc 之中第一个元素的最大值，第二个元素的最大值。。。 r: np.ones() 全是1的数组 s: np.convolve(weights,c) 卷积 t: np.exp(x) 指数函数 即e**x u: np.linspace(f,e,s)根据起始值，终止值，步长产生一个数组 v: a.fill(8) 将a的值全部设为8 w: np.linalg.lstsq(A,y)[0] 求解Ap = y p的值 求解多元一次方程组 x: np.dot(a,b) ａｂ的乘积 y: np.intersectld(a,b) ab数组的｀相同元素 z: a.clip(1,2) 修剪数组 比１小的都换成１，比２大的都换成２ a.compress(a&gt;2) 返回满足条件的数组元素 b.prod() b中各元素相乘 和arange结合可以求阶乘 b.cumprod 返回ｂ中各元素的累积乘积7:便捷函数: np.cov(X,Y) 求两矩阵的协方差 a.diagonal() 求对角线上的元素 a.trace() 求矩阵迹,对角线上元素之和 np.corrcoef(a,b) 相当于covariance / (a.std() * b.std()) 两向量相关系数为协方差除两标准差的乘积 np.ployfit(x,y,3) 拟合,用三次多项式 得到多项式的系数poly np.polyval(poly,x[-1]+1) 推断下一个值 np.roots(poly) 求多项式的根 np.polyder(poly) 求多项式导函数 np.sign() 返回元素符号,正为1 负为-1 np.piecewise() 分段给定值 如 np.piecewise(x, [x &lt; 0, x &gt;= 0], [lambda x: -x, lambda x: x]) 或: np.piecewise(x, [x &lt; 0, x &gt;= 0], [-1, 1]) np.array_equal(a,b) 检验两数组是否相同 vfunc=np.vertorize(func) 分别作用于一个函数,相当于Python的map函数 np.hanning(N) 计算权重,生成长度为N的窗口 np.polysub(a,b) 对多项式相减 np.isreal() 判断数组是否为实数 np.select(condlist, choicelist) 根据条件筛选8:矩阵和通用函数： A = np.mat('1 2 3;4 5 6;7 8 9') 创建矩阵 A.T A.I转置矩阵和逆矩阵 np.bmat(\"A B;A B\")创建分块复合矩阵 np.frompyfunc(func, nin, nout) 创建通用函数 nin:输入参数个数，nout输出对象个数 通用函数的四个方法： reduce 沿着指定的轴，递归作用于数组， accumulate 和reduce的区别是会返回中间值 reduceat outer np.divide(a,b) 除法，结果只保留整数部分 np.true_divide(a,b) 除法，结果保留小数 np.floor_divide() 除法，只保留整数结果，但加. np.remainder() 返回两数组相处后的余数 9:linalg模块 np.linalg,inv(a) 求逆矩阵 np.linalg.solve(A,b) 求解线性方程组 np.linalg.eigvals() 求特征值 m,n=np.linalg.eig(A) 求特征值和特征向量 np.linalg.pinv() 求广义逆矩阵 np.linalg.det(A) 计算矩阵的行列式 10：fft模块 np.fft.fft(a) 求傅里叶变换 np.fft.fftshift() 傅里叶变换后移频 np.fft.ifftshift() 移频逆操作 np.random.binomial(n,p,size) 计算二项式分布 np.random.hypergeometric(ngood, nbad, nsample, size=None) 超几何分布 np.random.normal(size=N) 正太分布 np.random.lognormal(size=N) 对数正太分布11：专用函数： np.searchsorted(a, v, side='left') 在数组a中插入值v，返回v在数组中的位置 np.lexsort(a,b) 先根据a，若相同根据b来排序。a,b按索引值对应。 np.i0() 第一类修正的零阶贝塞尔函数 np.sinc() sinc函数","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"socket摘录","slug":"socket摘录","date":"2016-08-16T14:31:37.000Z","updated":"2018-03-16T14:49:36.000Z","comments":true,"path":"2016/08/16/socket摘录/","link":"","permalink":"http://yoursite.com/2016/08/16/socket摘录/","excerpt":"可重用套接字当连接被有意无意关闭时，但此时端口还没有释放，要经过一个TIME_WAIT的过程之后才能使用。要想再次连接，或者说让多个套接字绑定在同一端口，可以通过socket.setsockopt函数来实现。","text":"可重用套接字当连接被有意无意关闭时，但此时端口还没有释放，要经过一个TIME_WAIT的过程之后才能使用。要想再次连接，或者说让多个套接字绑定在同一端口，可以通过socket.setsockopt函数来实现。123456789import sockets = socket.socket(socket.AF_INET,socket.SOCK_STREAM)oldstatus = s.getsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR)#打印旧的地址可重用状态print(\"old address status:\",oldstatus)s.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)newstatus = s.getsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR)#打印新的地址可重用状态print(\"new address status:%s\" %newstatus) 结果： 123old address status: 0new address status:1[Finished in 0.0s] 修改套接字发送和接收的缓存区大小代码： 1234567891011121314import socketSEND_BUF_SIZE = 4096RECV_BUF_SIZE = 4096def changeBufSize(): s = socket.socket(socket.AF_INET,socket.SOCK_STREAM) bufsize = s.getsockopt(socket.SOL_SOCKET,socket.SO_SNDBUF) print(\"buffer size before change:\",bufsize) s.setsockopt(socket.SOL_TCP,socket.TCP_NODELAY,1) s.setsockopt(socket.SOL_SOCKET,socket.SO_SNDBUF,SEND_BUF_SIZE) s.setsockopt(socket.SOL_SOCKET,socket.SO_RCVBUF,RECV_BUF_SIZE) bufsize2 = s.getsockopt(socket.SOL_SOCKET,socket.SO_SNDBUF) print('buffer size after change:',bufsize2)if __name__ == '__main__': changeBufSize() 结果： 12buffer size before change: 16384buffer size after change: 8192","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"Ubuntu开机自动挂载windows的分区","slug":"Ubuntu开机自动挂载windows的分区","date":"2016-06-03T14:26:03.000Z","updated":"2018-03-16T14:29:36.000Z","comments":true,"path":"2016/06/03/Ubuntu开机自动挂载windows的分区/","link":"","permalink":"http://yoursite.com/2016/06/03/Ubuntu开机自动挂载windows的分区/","excerpt":"电脑装的是ubuntu和win10双系统.装ubuntu的时候给ubuntu分配的磁盘小了,最近发现有点不够用.于是想着从window拿出一块盘来给linux用.查找资料,最后折腾成功啦.记录一下过程.","text":"电脑装的是ubuntu和win10双系统.装ubuntu的时候给ubuntu分配的磁盘小了,最近发现有点不够用.于是想着从window拿出一块盘来给linux用.查找资料,最后折腾成功啦.记录一下过程.window下的文件系统现在一般是ntfs.在linux其实是可以访问的.但感觉用起来有些不方便.先就是把window下某个盘如e盘(选e盘因为没有在e盘装任何软件)的东西全部备份一下.可以全部复制到别的盘.然后格式化e盘.点击我的电脑-&gt;管理-&gt;磁盘管理.找到e分区.右击删除卷.就把ntfs文件系统给删了.然后回到ubuntu中. 这时候需要用到一个工具,Gparted Partition Editor. 在ubuntu software center搜索安装即可.打开页面是这样子的. 找到刚才删除的那块盘.根据盘的大小就知道啦.然后右击-&gt;new新建一个盘…操作完成后点那个勾apply all operations.就可以啦.就可以得到一个新的分区.我的是/dev/sda 15. 后来发现上面的步骤更简单.备份数据后,直接用Gparted就可以格式话和format to ext4啦.不需要删除卷啥的. 接下来就是把这个分区挂载啦.我选择的是在 /mnt下新建一个目录wine 然后把盘挂载到这个目录下.. 通过blkid命令可以得到/dev/sda15 这样的信息/dev/sda15: UUID=”d05d23e1-ca9c-456d-b888-9cc3a0c5c627” TYPE=”ext4”复制UUID.然后vi /etc/fstab 编辑配置信息.在后面追加:我的是: 保存退出后.mount -a 就会根据配置文件自动挂载啦.而且开机重启后也不需要重新挂载.当然中间遇到了一些问题.如查看分区情况fdisk -l 会报错:WARNING: GPT (GUID Partition Table) detected on ‘/dev/sda’! The util fdisk doesn’t support GPT. Use GNU Parted.所以这个sda就是gpt的。看gpt分区详细内容可用parted，如：parted -l.还有涉及到挂载时分区的权限.和默认编码等问题.","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"socketserver源码学习之多进程和多线程Mixin","slug":"socketserver源码学习之多进程和多线程Mixin","date":"2016-05-08T14:42:13.000Z","updated":"2018-03-16T14:45:11.000Z","comments":true,"path":"2016/05/08/socketserver源码学习之多进程和多线程Mixin/","link":"","permalink":"http://yoursite.com/2016/05/08/socketserver源码学习之多进程和多线程Mixin/","excerpt":"最近看python标准库socketserver时，发现使用MixIn机制，通过自定义TCPServer或UDPServer时，继承一个socketserver.ThreadingMixIn类或是socketserver.ForkingMixIn类便可增加对请求的多线程或多进程处理。就像这样","text":"最近看python标准库socketserver时，发现使用MixIn机制，通过自定义TCPServer或UDPServer时，继承一个socketserver.ThreadingMixIn类或是socketserver.ForkingMixIn类便可增加对请求的多线程或多进程处理。就像这样 12class ThreadedTCPServer(socketserver.ForkingMixIn, socketserver.TCPServer): pass 非常的方便。 源码中对多线程的实现比较容易懂，通过对每个请求开一个线程去处理。代码如下： 123456def process_request(self, request, client_address): \"\"\"Start a new thread to process the request.\"\"\" t = threading.Thread(target = self.process_request_thread, args = (request, client_address)) t.daemon = self.daemon_threads t.start() 多进程的实现采取的不是multiprocessing模块，而是通过os模块的fork函数来产生一个子进程（在Unix/linux下，windows系统不提供fork函数，只能通过multiprocessing模块来实现多进程），通过子进程对新的请求进行处理。 os.fork()先来看看os模块的fork函数吧，Unix/Linux操作系统提供了一个fork()系统调用，它虽说也是函数，和普通函数却有点不一样，普通函数调用一次，返回一次，但是fork()函数调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。 子进程永远返回0，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用getppid()就可以拿到父进程的ID。 例子： 1234567import osprint('Process (%s) start...' % os.getpid())pid = os.fork()if pid == 0: print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))else: print('I am parent process (%s) child process (%s) is my child' % (os.getpid(), pid)) socketserver模块中，当一个新的请求发生时，通过调用os.fork()，当前进程产生一个子进程。然后通过子进程（pid为0)对请求进行处理，源代码如下： 1234567891011121314151617181920212223def process_request(self, request, client_address): \"\"\"Fork a new subprocess to process the request.\"\"\" pid = os.fork() if pid: # Parent process if self.active_children is None: self.active_children = set() self.active_children.add(pid) self.close_request(request) return else: # Child process. # This must never return, hence os._exit()! try: self.finish_request(request, client_address) self.shutdown_request(request) os._exit(0) except: try: self.handle_error(request, client_address) self.shutdown_request(request) finally: os._exit(1) 在子进程中处理新的连接请求，父进程则需要记录自己产生的子进程，将子进程的进程号添加到一个集合中，方便接下来处理，当子进程结束后，父进程调用wait/waitpid函数来释放子进程遗留在系统中的信息。这里又涉及到僵尸进程和孤儿进程的相关知识，网上查找一番后，才知道两者是什么。不是计算机专业出身，果然相关知识很欠缺。 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。在UNIX中,每个进程退出时，内核释放该进程的所有资源，包括打开的文件，占用的内存等， 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。 但这样就导致了问题，如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程，此即为僵尸进程的危害，应当避免。 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。孤儿进程是没有父进程的进程，init进程就成了孤儿进程的继父，当孤儿进程出现时，内核就把孤儿进程的父进程设置程init，而init进程会循环的wait()它的已经退出的子进程， 在socketserver中，产生子进程时，便会将子进程记录到一个集合中，子进程结束时，相当于系统中多了一个僵尸进程，socketserver通过collect_children函数来处理这时候僵尸进程。源代码： 12345678910111213141516171819202122232425262728293031323334max_children=40def collect_children(self): \"\"\"Internal routine to wait for children that have exited.\"\"\" if self.active_children is None: return # If we're above the max number of children, wait and reap them until # we go back below threshold. Note that we use waitpid(-1) below to be # able to collect children in size(&lt;defunct children&gt;) syscalls instead # of size(&lt;children&gt;): the downside is that this might reap children # which we didn't spawn, which is why we only resort to this when we're # above max_children. while len(self.active_children) &gt;= self.max_children: try: pid, _ = os.waitpid(-1, 0) self.active_children.discard(pid) except InterruptedError: pass except ChildProcessError: # we don't have any children, we're done self.active_children.clear() except OSError: break # Now reap all defunct children. for pid in self.active_children.copy(): try: pid, _ = os.waitpid(pid, os.WNOHANG) # if the child hasn't exited yet, pid will be 0 and ignored by # discard() below self.active_children.discard(pid) except ChildProcessError: # someone else reaped it self.active_children.discard(pid) except OSError: pass 这里有涉及到os.waitpid函数的使用， os.waitid(idtype, id, options)等待进程id为pid的进程结束，返回一个tuple，包括进程的进程ID和退出信息(和os.wait()一样)，参数options会影响该函数的行为。在默认情况下，options的值为0。如果pid是一个正数，waitpid()请求获取一个pid指定的进程的退出信息，如果pid为0，则等待并获取当前进程组中的任何子进程的值。如果pid为-1，则等待当前进程的任何子进程(相当于os.wait())，如果pid小于-1，则获取进程组id为pid的绝对值的任何一个进程。当系统调用返回-1时，抛出一个OSError异常。 waitpid()函数的options选项： os.WNOHANG - 如果没有子进程退出，则不阻塞waitpid()调用 os.WCONTINUED - 如果子进程从stop状态变为继续执行，则返回进程自前一次报告以来的信息。 os.WUNTRACED - 如果子进程被停止过而且其状态信息还没有报告过，则报告子进程的信息","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]},{"title":"collections模块学习总结","slug":"collections模块学习总结","date":"2016-03-16T14:19:03.000Z","updated":"2018-03-16T14:29:48.000Z","comments":true,"path":"2016/03/16/collections模块学习总结/","link":"","permalink":"http://yoursite.com/2016/03/16/collections模块学习总结/","excerpt":"Python内置的数据类型如list,tupple,dict已经非常强大啦,今天发现一个更强大的标准库:collections在原来的基础上有添加了几种额外的数据类型: Counter:计数器,dict的子类 deque:双端队列,可以快速的从首尾append和pop","text":"Python内置的数据类型如list,tupple,dict已经非常强大啦,今天发现一个更强大的标准库:collections在原来的基础上有添加了几种额外的数据类型: Counter:计数器,dict的子类 deque:双端队列,可以快速的从首尾append和pop Counter对象Counter是一个dict的子类,它也是无序存储,元素作为字典的keys,元素个数作为字典的value存储应用: 123456&gt;&gt;&gt; m = ['red','yellow','red','red','blue'] &gt;&gt;&gt; c = Counter(m) &gt;&gt;&gt; c Counter(&#123;'red': 3, 'yellow': 1, 'blue': 1&#125;) &gt;&gt;&gt; c['red'] 3 Counter的三种特有方法:elements()返回一个列举所有元素的迭代器,上代码: 1234&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2) &gt;&gt;&gt; list(c.elements())['a', 'a', 'a', 'a', 'b', 'b']most_common([n]) 列出计数最大的n个元素 12345&gt;&gt;&gt; Counter('abracadabra').most_common(3)[('a', 5), ('r', 2), ('b', 2)]&gt;&gt;&gt; Counter('abracadabra').most_common(4)[('a', 5), ('r', 2), ('b', 2), ('c', 1)]subtract([iterable-or-mapping]) 元素被扣除,从一个迭代器或另外一个mapping或计数器,有点像dict的update(),但不是那样取代元素 12345&gt;&gt;&gt; c = Counter(a=4, b=2, c=0, d=-2) &gt;&gt;&gt; d = Counter(a=1, b=2, c=3, d=4) &gt;&gt;&gt; c.subtract(d) &gt;&gt;&gt; c Counter(&#123;'a': 3, 'b': 0, 'c': -3, 'd': -6&#125;) 当然Counter是dict的子类,自然也支持dict的那些方法咯 deque类全称:“double-ended queue” 说白了就是双端队列,可以在头尾都进行添加和删除尽管list也支持类似的操作:如 pop(0)和 insert(0,v)但两者的时间复杂度不一样,list这样的操作是O(n),而deque为O(1) 方法:append(x) 末尾添加appendleft(x) 首部添加clear() 删除所有元素count(x) 统计x的个数extend(iterable) 在deque右边扩展,从itable中添加元素extendleft(iterable) 同理pop()popleft()reverse()反转remove(value) 一处第一个出现的value,后面的value保留rotate(n) 这个方法比较特殊,将deque往右移n位 如果n为负,则为左移deque(maxlen=10) 限制双端队列的长度,当满了的时候再添加,最老的元素会被删除,新元素添加进来 举个栗子 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; d = deque('ghi') # make a new deque with three items&gt;&gt;&gt; for elem in d: # iterate over the deque's elements... print elem.upper()GHI&gt;&gt;&gt; d.append('j') # add a new entry to the right side&gt;&gt;&gt; d.appendleft('f') # add a new entry to the left side&gt;&gt;&gt; d # show the representation of the dequedeque(['f', 'g', 'h', 'i', 'j'])&gt;&gt;&gt; d.pop() # return and remove the rightmost item'j'&gt;&gt;&gt; d.popleft() # return and remove the leftmost item'f'&gt;&gt;&gt; list(d) # list the contents of the deque['g', 'h', 'i']&gt;&gt;&gt; d[0] # peek at leftmost item'g'&gt;&gt;&gt; d[-1] # peek at rightmost item'i'&gt;&gt;&gt; list(reversed(d)) # list the contents of a deque in reverse['i', 'h', 'g']&gt;&gt;&gt; 'h' in d # search the dequeTrue&gt;&gt;&gt; d.extend('jkl') # add multiple elements at once&gt;&gt;&gt; ddeque(['g', 'h', 'i', 'j', 'k', 'l'])&gt;&gt;&gt; d.rotate(1) # right rotation&gt;&gt;&gt; ddeque(['l', 'g', 'h', 'i', 'j', 'k'])&gt;&gt;&gt; d.rotate(-1) # left rotation&gt;&gt;&gt; ddeque(['g', 'h', 'i', 'j', 'k', 'l'])&gt;&gt;&gt; deque(reversed(d)) # make a new deque in reverse orderdeque(['l', 'k', 'j', 'i', 'h', 'g'])&gt;&gt;&gt; d.clear() # empty the deque&gt;&gt;&gt; d.pop() # cannot pop from an empty dequeTraceback (most recent call last): File \"&lt;pyshell#6&gt;\", line 1, in -toplevel- d.pop()IndexError: pop from an empty deque&gt;&gt;&gt; d.extendleft('abc') # extendleft() reverses the input order&gt;&gt;&gt; ddeque(['c', 'b', 'a'])","categories":[],"tags":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/tags/编程/"}]}]}